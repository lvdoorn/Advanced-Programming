\documentclass{article}

\usepackage{syntax}

\title{Advanced Programming Assignment 2 Report}
\author{Mihai Popovici \texttt{MTF422} \and Leendert van Doorn \texttt{XBT504}}

\begin{document}
\maketitle

\section{Parser Combinator Library}
We have chosen to use the parser combinator library \texttt{parsec}. This choice is mainly motivated by the large amount of support available for \texttt{parsec} compared to \texttt{readP}. A Google search for ``haskell parsec" returns about 311.000 results, whereas a search for ``haskell readP" returns only 15.800 results. Furthermore, on StackOverflow, ``haskell readP" returns 8 results, compared to 1.360 for ``haskell parsec".

\section{Final Grammar}
% https://tex.stackexchange.com/questions/24886/which-package-can-be-used-to-write-bnf-grammars

\setlength{\grammarparsep}{4pt plus 1pt minus 1pt} % increase separation between rules
\setlength{\grammarindent}{9em} % increase separation between LHS/RHS 

\begin{grammar}
	\let\syntleft\relax
	\let\syntright\relax
	
	<Expr> ::= Expr1 Expr'
	
	<Expr'> ::= `,' Expr
	\alt $\varepsilon$

	<Expr1> ::= Assignable
	\alt Assignment
	\alt Ident `(' Exprs `)'
	\alt `[' Exprs `]'
	\alt `[' ArrayFor `]'
		
	<Factor> ::= Number
	\alt Ident
	\alt String
	\alt `true'
	\alt `false'
	\alt `undefined'
	\alt `(' Expr `)'
	
	<Term> ::= Factor Term'
	
	<Term'> ::= `ProdOp' Factor Term'
	\alt $\varepsilon$
	
	<ProdOp> ::= `*'
	\alt `\%'
	
	<Comparable> ::= Term Comparable'
	
	<Comparable'> ::= `AddOp' Term Comparable'
	\alt $\varepsilon$
	
	<AddOp> ::= `+'
	\alt `-'
	
	<Assignable> ::= Comparable Assignable'
	
	<Assignable'> ::= `CompOp' Comparable Assignable'
	\alt $\varepsilon$
	
	
	<CompOp> ::= `<'
	\alt `==='
	
	<Assignment> ::= Ident `=' Assignment'
	
	<Assignment'> ::= Expr1
	\alt Assignment
	
	<Exprs> ::= Expr1 CommaExprs
	\alt $\varepsilon$	
	
	<CommaExprs> ::= `,' Expr1 CommaExprs
	\alt $\varepsilon$
	
	<ArrayFor> ::= `for' `(' Ident `of' Expr1 `)' ArrayCompr
	
	<ArrayIf> ::= `if' `(' Expr1 `)' ArrayCompr
	
	<ArrayCompr> ::= Expr1
	\alt ArrayFor
	\alt ArrayIf
	
	<Ident> ::= (see below)
	
	<Number> ::= (see below)
	
	<String> ::= (see below)
\end{grammar}


\section{Handling of Whitespace}
We have chosen the solution of skipping whitespace after each token and at the very beginning, as was recommended in the lecture. To implement this, we have made a function \texttt{whitespace} which will take a parser as input, run the parser, and then skip any remaining whitespace and comments. In the top level function we skip whitespace and comments at the very beginning of the input.

\section{Capabilities of Implementation}
As far as we know our implementation will parse any input given to it correctly, and will fail on any invalid input. 

Of course, we cannot say this with 100\% certainty, we can only base our assessment on the test suite, as we will discuss now.


\section{Assessment of Implementation}

\subsection{Implementation Details}
In order to implement the parser, we first defined the grammar, which can be seen as a pseudo-code for the solution. In order to express all the possible legitimate expressions we used recursion. So, the algorithm is relative straightforward, given an expression, we recursively assess the given input with the rules we set based on the final grammar. We think that this approach is suitable in that it avoids using complex data structures and keeps things simple.

We have used the \texttt{try} combinator of the Parsec library in several cases. In these cases, it was necessary to use the try combinator because a parser that could fail might not fail on the first character. For example, when parsing the keyword \texttt{`for'}, when \texttt{`false'} is being parsed, the parser will fail, but after consuming the `f' character. This necessitates the use of the \texttt{try} combinator.

\subsection{Objective Assessment}
The overall quality of the code is high: 
\begin{itemize}
	\item code consists of small functions named accordingly;
	\item we separated code in different files. This way we separated various concerns in submodules;
	\item code is indented and readable;
	\item we used hlint, ghc -W and OnlineTA to clean up the code.
\end{itemize}
 
To assess the robustness of our implementation, we have written a test suite consisting of both unit and end-to-end tests.
We have a pretty high test coverage of the solution, namely 98 tests to prove correctness of implementation. The output of \texttt{stack test --coverage} is as follows: \newline
\texttt{
	95\% expressions used (500/522) \newline
	63\% boolean coverage (7/11)\newline
	50\% guards (4/8), 2 always True, 1 always False, 1 unevaluated\newline
	100\% 'if' conditions (3/3)\newline
	100\% qualifiers (0/0)\newline
	81\% alternatives used (13/16)\newline
	100\% local declarations used (3/3)\newline
	98\% top-level declarations used (53/54)\newline
	}
	
The Parsers were tested for concrete cases, whereas properties were tested using random generators (quickcheck). We have used quickcheck tests to generate random integers for numbers, and random strings for identifiers and strings. 

\subsection{Value Judgement}
Based on the coverage of our own test suite and the fact that our implementation passes all onlineTA tests, we can say that we are confident in the correct behaviour of our implementation. Although a test suite can never truly be exhaustive, we believe that we cover enough cases to argue that our implementation meets the requirements.

\section{How to Run the Code}
To run our code, execute \texttt{stack setup}, \texttt{stack install} and \texttt{stack test} in the top level \texttt{src} directory.

To generate a coverage report, execute \texttt{stack test --coverage}



\end{document}